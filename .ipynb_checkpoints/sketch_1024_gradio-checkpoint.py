import os
import torch
import gradio as gr
import numpy as np
from diffusers.pipelines import FluxPipeline
from PIL import Image, ImageDraw
import cv2

from omini.pipeline.flux_omini import Condition, generate, seed_everything

# å…¨å±€å˜é‡
pipe = None
edit_confirmed = False  # ç¼–è¾‘ç¡®è®¤çŠ¶æ€
current_edit_data = None  # å½“å‰ç¼–è¾‘æ•°æ®
last_sketch_hash = None  # ä¸Šæ¬¡ç¼–è¾‘æ•°æ®çš„å“ˆå¸Œå€¼ï¼Œç”¨äºæ£€æµ‹å˜åŒ–

def initialize_pipeline():
    """åˆå§‹åŒ–pipeline - å¼ºåˆ¶ä½¿ç”¨CUDAï¼Œè¿”å›çŠ¶æ€ä¿¡æ¯"""
    global pipe
    
    if pipe is not None:
        return True, "âœ… Pipelineå·²å°±ç»ª"
    
    print("ğŸ”„ æ­£åœ¨åˆå§‹åŒ–pipeline...")
    
    try:
        # å¼ºåˆ¶æ£€æŸ¥CUDAå¯ç”¨æ€§
        if not torch.cuda.is_available():
            error_msg = "âŒ CUDAä¸å¯ç”¨ï¼Œè¯·ç¡®ä¿GPUé©±åŠ¨æ­£ç¡®å®‰è£…"
            print(error_msg)
            return False, error_msg
        
        print(f"âœ… ä½¿ç”¨GPU: {torch.cuda.get_device_name()}")
        
        # NOTE: è¯·ä¿®æ”¹ä¸ºä½ çš„å®é™…æ¨¡å‹è·¯å¾„
        local_path = "/root/private_data/wangqiqi12/Omini_ckpts/FLUX.1-dev"
        
        print(f"ğŸ“‚ åŠ è½½åŸºç¡€æ¨¡å‹: {local_path}")
        # å¼ºåˆ¶ä½¿ç”¨CUDA - ä½¿ç”¨device_mapè‡ªåŠ¨å¤„ç†è®¾å¤‡æ”¾ç½®
        pipe = FluxPipeline.from_pretrained(
            local_path,
            torch_dtype=torch.bfloat16,
            device_map="cuda",  # è‡ªåŠ¨è®¾å¤‡æ˜ å°„ï¼Œä¼šä¼˜å…ˆä½¿ç”¨CUDA
            low_cpu_mem_usage=True,
        )
        
        # éªŒè¯æ¨¡å‹è®¾å¤‡ä¿¡æ¯
        try:
            if hasattr(pipe, 'device'):
                print(f"ğŸ” Pipelineè®¾å¤‡: {pipe.device}")
            elif hasattr(pipe, 'transformer') and hasattr(pipe.transformer, 'device'):
                print(f"ğŸ” Transformerè®¾å¤‡: {pipe.transformer.device}")
            else:
                print("ğŸ” è®¾å¤‡ä¿¡æ¯: ä½¿ç”¨device_mapè‡ªåŠ¨ç®¡ç†")
        except:
            print("ğŸ” è®¾å¤‡ä¿¡æ¯: è‡ªåŠ¨ç®¡ç†ä¸­")
        
        print("ğŸ“¦ åŠ è½½LoRAæƒé‡...")
        # NOTE: è¯·ä¿®æ”¹ä¸ºä½ çš„å®é™…LoRAè·¯å¾„  
        lora_path = "root/private_data/wangqiqi12/Omini_ckpts/omni_ckpts/only_sketch_1024"
        
        # æ£€æŸ¥LoRAæ–‡ä»¶æ˜¯å¦å­˜åœ¨
        import os
        lora_file = os.path.join(lora_path, "default.safetensors")
        if not os.path.exists(lora_file):
            error_msg = f"âŒ LoRAæ–‡ä»¶ä¸å­˜åœ¨: {lora_file}"
            print(error_msg)
            print("   è¯·æ£€æŸ¥è·¯å¾„æˆ–æƒé‡æ–‡ä»¶å")
            return False, error_msg
        
        pipe.load_lora_weights(
            lora_path,
            weight_name="default.safetensors",
            adapter_name="sketch",
        )
        
        # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼å¹¶ä¼˜åŒ–å†…å­˜
        pipe.unet.eval() if hasattr(pipe, 'unet') else None
        pipe.transformer.eval() if hasattr(pipe, 'transformer') else None
        
        # å¯ç”¨å†…å­˜ä¼˜åŒ–ï¼ˆä¸device_mapå…¼å®¹ï¼‰
        try:
            pipe.enable_attention_slicing()
            print("âœ… å·²å¯ç”¨æ³¨æ„åŠ›åˆ‡ç‰‡ä»¥èŠ‚çœå†…å­˜")
        except Exception as e:
            print(f"âš ï¸ æ³¨æ„åŠ›åˆ‡ç‰‡ä¸å¯ç”¨: {e}")
        
        # æ³¨æ„ï¼šä½¿ç”¨device_mapæ—¶ä¸å»ºè®®åŒæ—¶ä½¿ç”¨CPUå¸è½½
        print("ğŸ’¡ ä½¿ç”¨device_mapè‡ªåŠ¨ç®¡ç†å†…å­˜ï¼Œè·³è¿‡CPUå¸è½½")
        
        print("ğŸ‰ Pipelineåˆå§‹åŒ–å®Œæˆ!")
        return True, "âœ… Pipelineåˆå§‹åŒ–å®Œæˆ!"
        
    except FileNotFoundError as e:
        error_msg = f"âŒ æ–‡ä»¶æœªæ‰¾åˆ°: {str(e)}"
        print(error_msg)
        return False, error_msg
    except torch.cuda.OutOfMemoryError as e:
        error_msg = f"âŒ GPUå†…å­˜ä¸è¶³: {str(e)}"
        print(error_msg)
        print("ğŸ’¡ å»ºè®®: å°è¯•é‡å¯ç¨‹åºæˆ–ä½¿ç”¨æ›´å°çš„æ¨¡å‹")
        return False, error_msg
    except Exception as e:
        error_msg = f"âŒ Pipelineåˆå§‹åŒ–å¤±è´¥: {str(e)}"
        print(error_msg)
        return False, error_msg

def create_masked_image_from_sketch(base_image, sketch_data):
    """ä»baseå›¾åƒå’Œç”¨æˆ·ç¼–è¾‘çš„sketchåˆ›å»ºmaskedå›¾åƒ"""
    if base_image is None:
        return None, "è¯·å…ˆä¸Šä¼ åŸºç¡€å›¾åƒ"
    
    if sketch_data is None:
        return None, "è¯·å…ˆåœ¨å›¾åƒä¸Šç¼–è¾‘maskåŒºåŸŸ"
    
    try:
        # å¤„ç†baseå›¾åƒ
        if isinstance(base_image, np.ndarray):
            base_image = Image.fromarray(base_image.astype(np.uint8))
        base_image = base_image.resize((1024, 1024)).convert('RGB')
        
        # å¤„ç†ç”¨æˆ·ç¼–è¾‘åçš„å›¾åƒæ•°æ®
        edited_image = None
        
        if isinstance(sketch_data, dict):
            # å°è¯•å¸¸è§çš„å­—æ®µå - ä¼˜å…ˆä½¿ç”¨compositeï¼ˆåˆæˆå›¾åƒï¼‰
            possible_keys = ['composite', 'image', 'background', 'layers', 'data']
            for key in possible_keys:
                if key in sketch_data and sketch_data[key] is not None:
                    edited_image = sketch_data[key]
                    break
        elif isinstance(sketch_data, np.ndarray):
            edited_image = sketch_data
        elif isinstance(sketch_data, Image.Image):
            edited_image = sketch_data
        else:
            edited_image = sketch_data
        
        if edited_image is None:
            return None, "æ— æ³•ä»ç¼–è¾‘æ•°æ®ä¸­æå–å›¾åƒ"
        
        # ç»Ÿä¸€è½¬æ¢ä¸ºPILå›¾åƒ    
        if isinstance(edited_image, np.ndarray):
            if edited_image.dtype != np.uint8:
                edited_image = (edited_image * 255).astype(np.uint8) if edited_image.max() <= 1 else edited_image.astype(np.uint8)
            edited_image = Image.fromarray(edited_image)
        elif not isinstance(edited_image, Image.Image):
            return None, f"ä¸æ”¯æŒçš„å›¾åƒæ•°æ®ç±»å‹: {type(edited_image)}"
        
        # è°ƒæ•´å°ºå¯¸å’Œæ ¼å¼
        edited_image = edited_image.resize((1024, 1024)).convert('RGB')
        
        # åˆ›å»ºmaskï¼šæ£€æµ‹ç™½è‰²æ¶‚æŠ¹åŒºåŸŸ
        edited_array = np.array(edited_image)
        base_array = np.array(base_image)
        
        # æ‰¾åˆ°ç”¨æˆ·ç”¨ç™½è‰²ç¬”æ¶‚æŠ¹çš„åŒºåŸŸï¼ˆæ¥è¿‘ç™½è‰²çš„åƒç´ ï¼‰
        white_mask = ((edited_array[:,:,0] > 240) & 
                     (edited_array[:,:,1] > 240) & 
                     (edited_array[:,:,2] > 240))
        
        # åŒæ—¶æ’é™¤åŸå›¾ä¸­æœ¬æ¥å°±æ˜¯ç™½è‰²çš„åŒºåŸŸ
        original_white = ((base_array[:,:,0] > 240) & 
                         (base_array[:,:,1] > 240) & 
                         (base_array[:,:,2] > 240))
        
        # çœŸæ­£çš„maskåŒºåŸŸï¼šç¼–è¾‘åæ˜¯ç™½è‰²ä½†åŸå›¾ä¸æ˜¯ç™½è‰²çš„åŒºåŸŸ
        mask_region = white_mask & ~original_white
        
        # å¦‚æœæ²¡æœ‰æ£€æµ‹åˆ°æ˜æ˜¾çš„ç™½è‰²æ¶‚æŠ¹ï¼Œåˆ™æ£€æµ‹æ‰€æœ‰æ˜æ˜¾å˜åŒ–çš„åŒºåŸŸ
        if np.sum(mask_region) < 100:
            diff = np.abs(edited_array.astype(np.float32) - base_array.astype(np.float32))
            diff_magnitude = np.sum(diff, axis=2)
            mask_region = diff_magnitude > 30
        
        # åˆ›å»ºæœ€ç»ˆçš„maskedå›¾åƒ
        masked_image = edited_image.copy()
        
        # ç»Ÿè®¡maskåŒºåŸŸ
        mask_pixels = np.sum(mask_region)
        total_pixels = mask_region.size
        mask_percentage = (mask_pixels / total_pixels) * 100
        
        status = f"ç¼–è¾‘æ£€æµ‹æˆåŠŸ! MaskåŒºåŸŸ: {mask_pixels} åƒç´  ({mask_percentage:.1f}%)"
        
        return masked_image, status
        
    except Exception as e:
        return None, f"å¤„ç†ç¼–è¾‘å›¾åƒå¤±è´¥: {str(e)}"

def generate_image(prompt, num_steps, guidance_scale):
    """ç”Ÿæˆå›¾åƒçš„ä¸»å‡½æ•° - ä½¿ç”¨ç¡®è®¤çš„ç¼–è¾‘æ•°æ®"""
    global edit_confirmed, current_edit_data
    
    try:
        print(f"ğŸ”„ å¼€å§‹ç”Ÿæˆå›¾åƒ...")
        
        # æ£€æŸ¥ç¼–è¾‘ç¡®è®¤çŠ¶æ€
        if not edit_confirmed or current_edit_data is None:
            return None, None, "âŒ è¯·å…ˆç‚¹å‡»'ç¡®è®¤ç¼–è¾‘å®Œæˆ'æŒ‰é’®"
        
        base_image = current_edit_data['base_image']
        sketch_data = current_edit_data['sketch_data']
        
        if not prompt.strip():
            return None, None, "âŒ è¯·è¾“å…¥promptæè¿°"
        
        # åˆå§‹åŒ–pipeline - å¼ºåˆ¶CUDAæ£€æŸ¥
        print("â³ æ­£åœ¨åˆå§‹åŒ–æ¨¡å‹...")
        try:
            success, init_msg = initialize_pipeline()
            if not success:
                return None, None, init_msg
            print(init_msg)
        except Exception as init_error:
            print(f"Pipelineåˆå§‹åŒ–é”™è¯¯: {init_error}")
            return None, None, f"âŒ æ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {str(init_error)}"
        
        print("ğŸ–¼ï¸ ä½¿ç”¨å·²ç¡®è®¤çš„ç¼–è¾‘æ•°æ®...")
        # ç›´æ¥ä½¿ç”¨ç¡®è®¤çš„maskedå›¾åƒ
        try:
            masked_image = current_edit_data['masked_image']
            print("âœ… å·²ç¡®è®¤çš„ç¼–è¾‘æ•°æ®åŠ è½½æˆåŠŸ")
        except Exception as mask_error:
            print(f"ç¼–è¾‘æ•°æ®é”™è¯¯: {mask_error}")
            return None, None, f"âŒ ç¼–è¾‘æ•°æ®åŠ è½½å¤±è´¥: {str(mask_error)}"
        
        print("ğŸ¯ å‡†å¤‡ç”Ÿæˆæ¡ä»¶...")
        # åˆ›å»ºcondition - æ·»åŠ å®‰å…¨æ£€æŸ¥
        try:
            # ç¡®ä¿LoRAé€‚é…å™¨æ­£ç¡®åŠ è½½
            if hasattr(pipe, 'set_adapters'):
                pipe.set_adapters("sketch")
            condition = Condition(masked_image, "sketch")
        except Exception as condition_error:
            print(f"æ¡ä»¶å‡†å¤‡é”™è¯¯: {condition_error}")
            return None, None, f"âŒ æ¡ä»¶å‡†å¤‡å¤±è´¥: {str(condition_error)}"
        
        # è®¾ç½®éšæœºç§å­
        seed_everything(42)
        
        # ç”Ÿæˆå›¾åƒ - æ·»åŠ è¿›åº¦æç¤º
        print(f"ğŸš€ æ­£åœ¨ç”Ÿæˆå›¾åƒ (æ­¥æ•°: {int(num_steps)}, å¼•å¯¼å¼ºåº¦: {guidance_scale})")
        print(f"ğŸ“ Prompt: {prompt}")
        
        try:
            # ä½¿ç”¨torch.no_grad()ä¼˜åŒ–å†…å­˜ä½¿ç”¨
            with torch.no_grad():
                result = generate(
                    pipe,
                    prompt=prompt,
                    conditions=[condition],
                    height=1024,
                    width=1024,
                    num_inference_steps=int(num_steps),
                    guidance_scale=guidance_scale,
                )
            
            if result is None or len(result.images) == 0:
                return None, None, "âŒ ç”Ÿæˆå¤±è´¥ï¼šæ¨¡å‹è¿”å›ç©ºç»“æœ"
                
            result_img = result.images[0]
            print("âœ… å›¾åƒç”Ÿæˆå®Œæˆ")
            
        except Exception as gen_error:
            print(f"ç”Ÿæˆè¿‡ç¨‹é”™è¯¯: {gen_error}")
            # æ¸…ç†GPUå†…å­˜
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
            return None, None, f"âŒ ç”Ÿæˆå¤±è´¥: {str(gen_error)}"
        
        print("ğŸ”— æ­£åœ¨åˆ›å»ºå¯¹æ¯”å›¾...")
        # åˆ›å»ºå¯¹æ¯”å›¾åƒ - ä¼˜åŒ–å†…å­˜ä½¿ç”¨
        try:
            concat_image = Image.new("RGB", (1024 * 3, 1024))
            base_resized = base_image.resize((1024, 1024)).convert('RGB') if base_image else Image.new("RGB", (1024, 1024), (255, 255, 255))
            concat_image.paste(base_resized, (0, 0))
            concat_image.paste(masked_image, (1024, 0))
            concat_image.paste(result_img, (1024 * 2, 0))
        except Exception as concat_error:
            print(f"å¯¹æ¯”å›¾åˆ›å»ºé”™è¯¯: {concat_error}")
            # å³ä½¿å¯¹æ¯”å›¾å¤±è´¥ï¼Œä¹Ÿè¿”å›ç”Ÿæˆç»“æœ
            return result_img, None, "âš ï¸ ç”ŸæˆæˆåŠŸï¼Œä½†å¯¹æ¯”å›¾åˆ›å»ºå¤±è´¥"
        
        # ä¼˜åŒ–ä¿å­˜é€»è¾‘ - å¼‚æ­¥ä¿å­˜ï¼Œé¿å…é˜»å¡GradioçŠ¶æ€
        try:
            import threading
            
            def save_images_async():
                try:
                    os.makedirs("gradio_output", exist_ok=True)
                    result_img.save("gradio_output/result.jpg", quality=95, optimize=True)
                    concat_image.save("gradio_output/comparison.jpg", quality=95, optimize=True)
                    print("ğŸ’¾ ç»“æœå·²å¼‚æ­¥ä¿å­˜åˆ° gradio_output/")
                except Exception as save_error:
                    print(f"å¼‚æ­¥ä¿å­˜é”™è¯¯: {save_error}")
            
            # å¯åŠ¨åå°ä¿å­˜çº¿ç¨‹ï¼Œä¸é˜»å¡ä¸»æµç¨‹
            save_thread = threading.Thread(target=save_images_async, daemon=True)
            save_thread.start()
            
        except Exception as save_error:
            print(f"ä¿å­˜çº¿ç¨‹å¯åŠ¨å¤±è´¥: {save_error}")
            # ä¿å­˜å¤±è´¥ä¸å½±å“è¿”å›ç»“æœ
        
        # æ¸…ç†GPUå†…å­˜
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        
        return result_img, concat_image, "ğŸ‰ ç”ŸæˆæˆåŠŸï¼"
        
    except Exception as e:
        error_msg = f"âŒ ç”Ÿæˆå¤±è´¥: {str(e)}"
        print(error_msg)
        # å‘ç”Ÿé”™è¯¯æ—¶æ¸…ç†GPUå†…å­˜
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        return None, None, error_msg

def update_sketch_pad(base_image):
    """å½“ä¸Šä¼ æ–°å›¾åƒæ—¶ï¼Œæ›´æ–°ç»˜åˆ¶ç”»å¸ƒçš„èƒŒæ™¯å¹¶é‡ç½®ç¼–è¾‘çŠ¶æ€"""
    global edit_confirmed, current_edit_data, last_sketch_hash
    
    # é‡ç½®ç¼–è¾‘çŠ¶æ€
    edit_confirmed = False
    current_edit_data = None
    last_sketch_hash = None
    
    if base_image is None:
        return np.ones((1024, 1024, 3), dtype=np.uint8) * 255  # ç™½è‰²èƒŒæ™¯
    
    # å°†PILå›¾åƒè½¬æ¢ä¸ºnumpyæ•°ç»„
    if isinstance(base_image, Image.Image):
        # è°ƒæ•´åˆ°1024 * 1024å¹¶è½¬æ¢ä¸ºnumpyæ•°ç»„
        resized_image = base_image.resize((1024, 1024)).convert('RGB')
        return np.array(resized_image)
    elif isinstance(base_image, np.ndarray):
        return base_image
    else:
        return np.ones((1024, 1024, 3), dtype=np.uint8) * 255

def check_sketch_changes(sketch_data):
    """æ£€æµ‹ç¼–è¾‘åŒºåŸŸæ˜¯å¦æœ‰å˜åŒ–ï¼Œå¹¶é‡ç½®ç¡®è®¤çŠ¶æ€"""
    global edit_confirmed, last_sketch_hash
    import hashlib
    
    if sketch_data is None:
        return "â³ ç­‰å¾…ç¼–è¾‘..."
    
    try:
        # è®¡ç®—å½“å‰ç¼–è¾‘æ•°æ®çš„å“ˆå¸Œå€¼
        if isinstance(sketch_data, dict) and 'composite' in sketch_data:
            data_to_hash = sketch_data['composite']
        else:
            data_to_hash = sketch_data
            
        if isinstance(data_to_hash, np.ndarray):
            current_hash = hashlib.md5(data_to_hash.tobytes()).hexdigest()
        else:
            current_hash = str(hash(str(data_to_hash)))
        
        # æ£€æŸ¥æ˜¯å¦æœ‰å˜åŒ–
        if last_sketch_hash != current_hash:
            if last_sketch_hash is not None:
                edit_confirmed = False
                return "ğŸ”„ æ£€æµ‹åˆ°ç¼–è¾‘å˜åŒ–ï¼Œè¯·é‡æ–°ç¡®è®¤ç¼–è¾‘"
            last_sketch_hash = current_hash
            
        if edit_confirmed:
            return "âœ… ç¼–è¾‘å·²ç¡®è®¤"
        else:
            return "â³ è¯·ç‚¹å‡»ç¡®è®¤ç¼–è¾‘æŒ‰é’®"
            
    except Exception as e:
        return f"âš ï¸ çŠ¶æ€æ£€æŸ¥é”™è¯¯: {str(e)}"

def continue_editing(base_image):
    """ç»§ç»­ç¼–è¾‘åŠŸèƒ½ - é‡ç½®æ‰€æœ‰çŠ¶æ€å¹¶è¿”å›ç¼–è¾‘æ¨¡å¼"""
    global edit_confirmed, current_edit_data, last_sketch_hash
    
    # é‡ç½®æ‰€æœ‰ç¼–è¾‘ç›¸å…³çŠ¶æ€
    edit_confirmed = False
    current_edit_data = None
    last_sketch_hash = None
    
    # æ¸…ç†GPUå†…å­˜
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
    
    # å‡†å¤‡è¿”å›ç¼–è¾‘æ¨¡å¼çš„çŠ¶æ€ä¿¡æ¯
    main_status = "ğŸ”„ å·²é‡ç½®ï¼Œè¯·é‡æ–°ç¼–è¾‘å’Œç¡®è®¤"
    
    # æ›´æ–°ç¼–è¾‘ç”»å¸ƒèƒŒæ™¯
    sketch_pad_result = update_sketch_pad(base_image)
    
    return sketch_pad_result, main_status

def confirm_edit_ready(base_image, sketch_data):
    """ç¡®è®¤ç¼–è¾‘å°±ç»ª"""
    global edit_confirmed, current_edit_data, last_sketch_hash
    
    import hashlib
    
    # å¼ºåˆ¶é‡ç½®çŠ¶æ€ï¼Œç¡®ä¿æ¯æ¬¡éƒ½æ˜¯æ–°çš„ç¡®è®¤
    edit_confirmed = False
    current_edit_data = None
    
    # ç«‹å³åé¦ˆç”¨æˆ·æ“ä½œå·²æ”¶åˆ°
    if base_image is None:
        return "âŒ è¯·å…ˆä¸Šä¼ åŸºç¡€å›¾åƒ"
    
    if sketch_data is None:
        return "âŒ è¯·å…ˆåœ¨å›¾åƒä¸Šè¿›è¡Œç¼–è¾‘"
    
    # è®¡ç®—å½“å‰ç¼–è¾‘æ•°æ®çš„å“ˆå¸Œå€¼ï¼Œç”¨äºæ£€æµ‹å˜åŒ–
    try:
        if isinstance(sketch_data, np.ndarray):
            current_hash = hashlib.md5(sketch_data.tobytes()).hexdigest()[:8]
        else:
            current_hash = hashlib.md5(str(sketch_data).encode()).hexdigest()[:8]
    except:
        current_hash = "unknown"
    
    try:
        # å¿«é€ŸéªŒè¯ç¼–è¾‘æ•°æ®
        masked_image, status = create_masked_image_from_sketch(base_image, sketch_data)
        
        if masked_image is None:
            edit_confirmed = False
            return f"âŒ ç¼–è¾‘æ•°æ®æ— æ•ˆ: {status}"
        
        # å¼ºåˆ¶æ›´æ–°çŠ¶æ€
        edit_confirmed = True
        last_sketch_hash = current_hash
        current_edit_data = {
            'base_image': base_image,
            'sketch_data': sketch_data,
            'masked_image': masked_image,
            'hash': current_hash
        }
        
        return f"âœ… ç¼–è¾‘å·²ç¡®è®¤ï¼(å“ˆå¸Œ:{current_hash}) {status}"
        
    except Exception as e:
        edit_confirmed = False
        return f"âŒ ç¡®è®¤ç¼–è¾‘å¤±è´¥: {str(e)}"



# åˆ›å»ºGradioç•Œé¢
def create_ui():
    with gr.Blocks(title="OminiControl Inpainting Demo", theme=gr.themes.Soft()) as demo:
        gr.Markdown("# ğŸ¨ OminiControl Inpainting Demo")
        gr.Markdown("**ä½¿ç”¨è¯´æ˜**: ä¸Šä¼ å›¾åƒ â†’ ç¼–è¾‘ â†’ ç¡®è®¤ â†’ ç”Ÿæˆå›¾åƒ")
        
        with gr.Row():
            # ç«–ç›´å¸ƒå±€ï¼šä¸Šä¼ å›¾åƒåŒºåŸŸ
            with gr.Column(scale=1):
                base_image = gr.Image(
                    label="ğŸ“¤ 1. ä¸Šä¼ åŸºç¡€å›¾åƒ",
                    type="pil",
                    height=768,
                    width=768
                )
            # ç¼–è¾‘åŒºåŸŸ
            with gr.Column(scale=1):
                sketch_pad = gr.ImageEditor(
                    label="ğŸ–Œï¸ 2. åœ¨åŸå›¾ä¸Šç¼–è¾‘ (ç™½ç¬”æ¶‚æŠ¹maskåŒºåŸŸï¼Œé»‘ç¬”å‹¾å‹’sketch)",
                    type="numpy",
                    height=768,
                    brush=gr.Brush(
                    default_size=15,
                    colors=["#FFFFFF", "#000000"],
                    default_color="#FFFFFF"
                    ),
                    value=np.ones((1024, 1024, 3), dtype=np.uint8) * 255
                )
        
        # æ§åˆ¶æŒ‰é’®åŒºåŸŸ 
        with gr.Row():
            clear_btn = gr.Button("ğŸ—‘ï¸ é‡ç½®ä¸ºåŸå›¾", variant="secondary", size="sm")
            confirm_btn = gr.Button("âœ… ç¡®è®¤ç¼–è¾‘", variant="primary", size="sm")
        
        # å‚æ•°æ§åˆ¶åŒºåŸŸ
        with gr.Row():
            with gr.Column(scale=2):
                prompt = gr.Textbox(
                    label="âœï¸ 3. è¾“å…¥Promptæè¿°",
                    placeholder="æè¿°ä½ æƒ³è¦åœ¨maskåŒºåŸŸç”Ÿæˆçš„å†…å®¹ï¼Œä¾‹å¦‚ï¼šA beautiful flower vase",
                    lines=2,
                    value="A beautiful vase"
                )
            with gr.Column(scale=1):
                num_steps = gr.Slider(
                    minimum=10,
                    maximum=50,
                    value=28,
                    step=1,
                    label="æ¨ç†æ­¥æ•°"
                )
            with gr.Column(scale=1):
                guidance_scale = gr.Slider(
                    minimum=1.0,
                    maximum=10.0,
                    value=3.5,
                    step=0.1,
                    label="å¼•å¯¼å¼ºåº¦"
                )
        
        # ç”ŸæˆæŒ‰é’®
        with gr.Row():
            generate_btn = gr.Button("ğŸš€ ç”Ÿæˆå›¾åƒ", variant="primary", size="lg")
        
        # çŠ¶æ€æ˜¾ç¤º
        with gr.Row():
            status_text = gr.Textbox(
                label="ğŸ“ çŠ¶æ€ä¿¡æ¯",
                value="è¯·ä¸Šä¼ å›¾åƒå¹¶ç»˜åˆ¶maskåŒºåŸŸ",
                interactive=False,
                lines=1
            )
        
        # ç»“æœæ˜¾ç¤ºåŒºåŸŸ - æ”¹ä¸ºç«–ç›´å¸ƒå±€
        gr.Markdown("### ğŸ“Š ç»“æœå±•ç¤º")
        
        with gr.Tabs():
            with gr.TabItem("ğŸ“¸ ç”Ÿæˆç»“æœ"):
                output_image = gr.Image(
                    label="ç”Ÿæˆçš„å›¾åƒ",
                    type="pil",
                    height=600,  # å¢å¤§æ˜¾ç¤ºé«˜åº¦
                    width=600
                )
                # åœ¨ç”Ÿæˆç»“æœä¸‹æ–¹æ·»åŠ ç»§ç»­ç¼–è¾‘æŒ‰é’®
                with gr.Row():
                    continue_edit_btn = gr.Button("ğŸ”„ ç»§ç»­ç¼–è¾‘", variant="primary", size="lg")
            
            with gr.TabItem("ï¿½ å¯¹æ¯”å›¾"):
                comparison_image = gr.Image(
                    label="å¯¹æ¯”å›¾ (åŸå›¾|ç¼–è¾‘å›¾|ç”Ÿæˆç»“æœ)",
                    type="pil", 
                    height=400,  # å¯¹æ¯”å›¾ç¨å°ä¸€äº›ï¼Œå› ä¸ºæ˜¯æ¨ªå‘æ‹¼æ¥çš„
                )
        
        
        # ä½¿ç”¨è¯´æ˜
        with gr.Accordion("ğŸ“– è¯¦ç»†ä½¿ç”¨è¯´æ˜", open=False):
            gr.Markdown("""
            ### æ­¥éª¤è¯´æ˜:
            1. **ä¸Šä¼ å›¾åƒ**: é€‰æ‹©ä½ æƒ³è¦ç¼–è¾‘çš„åŸºç¡€å›¾åƒ
            2. **åœ¨åŸå›¾ä¸Šç¼–è¾‘**: 
               - ä½¿ç”¨**ç™½è‰²ç”»ç¬”**æ¶‚æŠ¹éœ€è¦ä¿®å¤/æ›¿æ¢çš„åŒºåŸŸï¼ˆmaskåŒºåŸŸï¼‰
               - ä½¿ç”¨**é»‘è‰²ç»†ç”»ç¬”**åœ¨maskåŒºåŸŸå†…å‹¾å‹’ä½ æƒ³è¦çš„å†…å®¹è½®å»“
            3. **ç¡®è®¤ç¼–è¾‘**: ç‚¹å‡»"âœ… ç¡®è®¤ç¼–è¾‘"æŒ‰é’®ä¿å­˜ç¼–è¾‘æ•°æ®
            4. **è¾“å…¥prompt**: è¯¦ç»†æè¿°ä½ æƒ³åœ¨ç¼–è¾‘åŒºåŸŸç”Ÿæˆçš„å†…å®¹
            5. **è°ƒæ•´å‚æ•°**: 
               - æ¨ç†æ­¥æ•°: å»ºè®®20-30ï¼Œæ›´å¤šæ­¥æ•°è´¨é‡æ›´å¥½ä½†é€Ÿåº¦æ›´æ…¢
               - å¼•å¯¼å¼ºåº¦: å»ºè®®3-5ï¼Œæ§åˆ¶ç”Ÿæˆå†…å®¹ä¸promptçš„ç›¸å…³æ€§
            6. **ç”Ÿæˆå›¾åƒ**: ç‚¹å‡»"ğŸš€ ç”Ÿæˆå›¾åƒ"æŒ‰é’®å¼€å§‹å¤„ç†
            7. **ç»§ç»­ç¼–è¾‘**: ç”Ÿæˆå®Œæˆåï¼Œç‚¹å‡»"ğŸ”„ ç»§ç»­ç¼–è¾‘"æŒ‰é’®é‡æ–°ç¼–è¾‘
            
            ### æ³¨æ„äº‹é¡¹:
            - ç¡®ä¿å·²æ­£ç¡®é…ç½®æ¨¡å‹è·¯å¾„ï¼ˆåœ¨ä»£ç ä¸­ä¿®æ”¹local_pathå’Œlora_pathï¼‰
            - å¿…é¡»å…ˆç”¨ç™½ç¬”æ¶‚æŠ¹åŒºåŸŸï¼Œå†ç”¨é»‘ç¬”å‹¾å‹’ç»†èŠ‚
            - **å¿…é¡»ç‚¹å‡»"ç¡®è®¤ç¼–è¾‘"æŒ‰é’®**æ‰èƒ½è¿›è¡Œç”Ÿæˆ
            - ç¼–è¾‘åçš„å›¾åƒï¼ˆåŒ…å«ç™½è‰²maskå’Œé»‘è‰²sketchï¼‰å°†ä½œä¸ºæ¡ä»¶å›¾è¾“å…¥æ¨¡å‹
            - å¼ºåˆ¶ä½¿ç”¨CUDAï¼Œç¡®ä¿GPUé©±åŠ¨æ­£ç¡®å®‰è£…
            """)
        
        # äº‹ä»¶ç»‘å®š - æ·»åŠ æ›´å¥½çš„ç”¨æˆ·ä½“éªŒ
        
        # å½“ä¸Šä¼ æ–°å›¾åƒæ—¶ï¼Œè‡ªåŠ¨æ›´æ–°ImageEditorçš„èƒŒæ™¯
        base_image.change(
            fn=update_sketch_pad,
            inputs=base_image,
            outputs=sketch_pad,
            show_progress="hidden"
        )
        
        # é‡ç½®æŒ‰é’®
        def reset_and_update_status(base_img):
            """é‡ç½®å¹¶æ›´æ–°çŠ¶æ€"""
            global edit_confirmed, current_edit_data
            edit_confirmed = False
            current_edit_data = None
            result = update_sketch_pad(base_img)
            return result, "ğŸ”„ å·²é‡ç½®ä¸ºåŸå›¾ï¼Œè¯·é‡æ–°ç¼–è¾‘"
        
        clear_btn.click(
            fn=reset_and_update_status,
            inputs=base_image,
            outputs=[sketch_pad, status_text],
            show_progress="hidden"
        )
        
        
        # ç¡®è®¤ç¼–è¾‘æŒ‰é’®
        confirm_btn.click(
            fn=confirm_edit_ready,
            inputs=[base_image, sketch_pad],
            outputs=status_text,
            show_progress="minimal"
        )
        
        # ç”ŸæˆæŒ‰é’®
        def safe_generate_image_with_status(prompt_text, num_steps, guidance_scale):
            """å®‰å…¨çš„ç”Ÿæˆå›¾åƒåŒ…è£…å‡½æ•°"""
            try:
                if not edit_confirmed:
                    return None, None, "âŒ è¯·å…ˆç‚¹å‡»'ç¡®è®¤ç¼–è¾‘'æŒ‰é’®"
                
                result_img, comparison_img, main_status = generate_image(prompt_text, num_steps, guidance_scale)
                return result_img, comparison_img, main_status
                
            except Exception as e:
                return None, None, f"âŒ ç”Ÿæˆè¿‡ç¨‹å‡ºé”™: {str(e)}"
        
        generate_event = generate_btn.click(
            fn=safe_generate_image_with_status,
            inputs=[prompt, num_steps, guidance_scale],
            outputs=[output_image, comparison_image, status_text],
            show_progress=True,
            scroll_to_output=True,
        )
        
        # ç»§ç»­ç¼–è¾‘æŒ‰é’® - é‡ç½®çŠ¶æ€å¹¶è¿”å›ç¼–è¾‘æ¨¡å¼
        continue_edit_btn.click(
            fn=continue_editing,
            inputs=base_image,
            outputs=[sketch_pad, status_text],
            show_progress="hidden"
        )
    
    return demo

if __name__ == "__main__":
    print("ğŸš€ å¯åŠ¨OminiControl Inpainting Demo...")
    
    # å¼ºåˆ¶æ£€æŸ¥CUDAå¯ç”¨æ€§
    if torch.cuda.is_available():
        print(f"âœ… CUDAå¯ç”¨ï¼ŒGPU: {torch.cuda.get_device_name()}")
    else:
        print("âŒ CUDAä¸å¯ç”¨ï¼Œç¨‹åºå°†æ— æ³•æ­£å¸¸å·¥ä½œ")
        exit(1)
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs("gradio_output", exist_ok=True)
    
    # å¯åŠ¨ç•Œé¢
    demo = create_ui()
    demo.launch(
        server_name="0.0.0.0",
        server_port=7860,
        share=False,
        debug=True,
        show_error=True
    )